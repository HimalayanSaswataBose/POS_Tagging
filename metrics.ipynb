{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoMVr1EeQ+9YW44Trkr8cQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimalayanSaswataBose/POS_Tagging/blob/main/metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYO3HcjLc5jh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_MIRA.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf16\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_MIRA.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O2G_MdVrfjs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_CRF.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf16\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_CRF.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YHDsOBu7aeP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_HMM.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_HMM.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niNQlARW7ap_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_sample1.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM1.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_ga2PsrRHM1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_sample2.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM2.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RdaHWfbSKxF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_sample3.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM3.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vO4Hsx5SJzU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_sample4.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM4.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-zOC41-SIpP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_sample5.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM5.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1G8UDktUc8O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_sampling.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM_sampling.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_Adadelta.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM_Adadelta.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ],
      "metadata": {
        "id": "L9bokqH78eRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_Adagrad.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM_Adagrad.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ],
      "metadata": {
        "id": "TgqSca1Z8u5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "result_file = \"res_LSTM_Adam.txt\"\n",
        "res_list = []\n",
        "actual = []\n",
        "predicted = []\n",
        "with open(result_file, encoding = \"utf8\", mode = \"r\") as f:\n",
        "  read = f.read().split('\\n')\n",
        "  for i in read:\n",
        "    if(i.split('\\t') != ['']):\n",
        "      res_list.append(i.split('\\t')[1:])\n",
        "df = pd.DataFrame(res_list, columns = ['Actual', 'Predicted'])\n",
        "df.to_csv('preds_LSTM_Adam.txt', sep = '\\t', index = False, header = False)\n",
        "for i in res_list:\n",
        "  actual.append(i[0])\n",
        "  predicted.append(i[1])\n",
        "display(df)"
      ],
      "metadata": {
        "id": "3ObfRtQY8zgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# Function to read predicted and gold tags from the same file\n",
        "def read_tags_from_file(file_path):\n",
        "    gold_tags = []\n",
        "    predicted_tags = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.strip():  # Check if the line is not empty\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) == 2:  # Ensure there are three parts to unpack\n",
        "                    gold_tag, predicted_tag = parts\n",
        "                    gold_tags.append(gold_tag)\n",
        "                    predicted_tags.append(predicted_tag)\n",
        "                else:\n",
        "                    print(\"Skipping line: \", line)  # Print a message for debugging\n",
        "    return gold_tags, predicted_tags\n",
        "\n",
        "# Function to calculate Precision, Recall, and F1 scores, and generate confusion matrix\n",
        "def evaluate_pos_tags(gold_tags, predicted_tags):\n",
        "    # Calculate Precision, Recall, and F1 scores\n",
        "    precision = precision_score(gold_tags, predicted_tags, average='weighted', zero_division = 1)\n",
        "    recall = recall_score(gold_tags, predicted_tags, average='weighted', zero_division = 1)\n",
        "    f1 = f1_score(gold_tags, predicted_tags, average='weighted', zero_division = 1)\n",
        "    acc = accuracy_score(gold_tags, predicted_tags)\n",
        "    report = classification_report(gold_tags, predicted_tags)\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(gold_tags, predicted_tags)\n",
        "\n",
        "    return acc, precision, recall, f1, cm, report\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n"
      ],
      "metadata": {
        "id": "ILfhNxAu3_MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_MIRA.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "5tp40qkudDOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_HMM.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "8o_LL7n68QhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_CRF.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "Zv_E_a_J6XfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM1.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "lL2sLiKI8bhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM2.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "QmgPnXEBRUOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM3.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "m_B4EOtXRUS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM4.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "UwDAdA3hRUWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM5.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "u5kNnWc_RUJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM_sampling.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "eTg72lTpU8fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM_Adadelta.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "Z9UCbLt-8-Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM_Adagrad.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "t_x1atE_9Api"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"preds_LSTM_Adam.txt\"  # Path to the file containing both gold and predicted tags\n",
        "gold_tags, predicted_tags = read_tags_from_file(file_path)\n",
        "acc, precision, recall, f1, cm, report = evaluate_pos_tags(gold_tags, predicted_tags)\n",
        "\n",
        "# Define class labels (unique POS tags)\n",
        "class_labels = sorted(set(gold_tags + predicted_tags))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "plot_confusion_matrix(cm, classes=class_labels, title='Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "print(\"Report:\\n\", report)"
      ],
      "metadata": {
        "id": "QBETwvmC9BiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}